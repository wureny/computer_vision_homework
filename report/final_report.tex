\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{booktabs}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}

\begin{document}

\title{Meme Template Classification Using Transfer Learning (MobileNetV2)}

\author{Wu RenYu; Zhou Ziqi}  % TODO: replace with your team member names + IDs

\markboth{The final project report of computer vision, September 2025}%
{Wu \MakeLowercase{\textit{et al.}}: Meme Template Classification Using Transfer Learning}

\IEEEpubid{Macau University of Science and Technology, CS460/ EIE460/ SE460}
\maketitle

\input{results.tex}

\begin{abstract}
Memes are widely used in online communities and can convey sentiment quickly through images. This project studies a lightweight image classification task: given a meme image, predict its template category. We adopt transfer learning with a MobileNetV2 backbone pretrained on ImageNet and fine-tune it on a small labeled dataset (3 classes, 90 images) with standard data augmentation. Experiments report an accuracy of \ResultAccuracy{} and a macro-F1 of \ResultMacroFone{} on a held-out test split, and we visualize a confusion matrix to analyze failure cases. GitHub URL: \url{<PUT_YOUR_GITHUB_URL_HERE>}
\end{abstract}

\begin{IEEEkeywords}
Image classification, transfer learning, MobileNetV2, meme template, computer vision.
\end{IEEEkeywords}

\section{Introduction}
This work aims to classify meme images into a set of predefined template categories (e.g., Pepe, Doge, Wojak). The main challenges are high intra-class variation (different captions, crops, redraw styles) and limited labeled data. We implement a simple and reproducible transfer learning baseline based on MobileNetV2 to achieve reasonable performance with low computational cost.

\section{Related work}
Meme template recognition can be formulated as a standard image classification problem, but it often exhibits traits of \emph{fine-grained} recognition: intra-class variation is large (captions, redraw styles, crops) while inter-class differences may be subtle (similar facial contours or textures). Convolutional neural networks (CNNs) remain a strong baseline for such visual categorization tasks. Classic backbones such as ResNet \cite{resnet} and more recent efficient architectures (e.g., EfficientNet \cite{efficientnet}) are commonly used as feature extractors, typically followed by a linear classifier for the downstream labels.

When labeled data is limited, transfer learning from large-scale pretrained models (most commonly ImageNet) is widely adopted. In this setting, one either (i) freezes the backbone and trains only a small classifier head, or (ii) fine-tunes part or all of the backbone to adapt features to the target domain. In our experiments we explicitly compare these two strategies (Table~\ref{tab:ablation}), which is consistent with common practice in small-data classification.

Beyond supervised CNN transfer learning, two additional directions are related. First, metric learning methods (e.g., Siamese-style or triplet-loss training) aim to learn embeddings where visually similar instances are close, which can be beneficial when class boundaries are ambiguous or when only a few examples per class are available \cite{siamese}. Second, large-scale vision transformers and vision-language models provide strong pretrained representations; models such as ViT \cite{vit} and CLIP \cite{clip} can be used for feature extraction or even zero-shot classification, offering an attractive alternative when collecting labeled data is difficult. In this project, we focus on a lightweight and reproducible supervised baseline based on MobileNetV2 \cite{mobilenetv2} implemented via TorchVision \cite{torchvision}.

\section{Technical Solution}
We use MobileNetV2 pretrained on ImageNet and replace the final classifier layer with a linear layer of size $C$ (the number of classes). Input images are resized to $224 \times 224$ and normalized with ImageNet statistics. During training, we apply augmentation including random horizontal flip, small rotation, and color jitter. The model is optimized with cross-entropy loss and Adam.

\subsection{Implementation details}
The project is implemented in PyTorch/TorchVision. Datasets follow the ImageFolder convention. We use a fixed random seed for data splitting. Training logs, checkpoints, and evaluation outputs are saved under \texttt{outputs/}. The best validation checkpoint is copied to \texttt{pretrained/mobilenetv2\_meme\_best.pt} for convenient testing.

\section{Experiments}
\subsection{Dataset}
Images are organized into folders by class and split into train/validation/test sets. The dataset used in this report contains \ResultNumClasses{} classes: \ResultClasses{}. We use a 0.6/0.2/0.2 split to keep a slightly larger test set. The dataset is still small (and slightly imbalanced across classes), and may contain biases (e.g., source website styles); we discuss limitations in Section~\ref{sec:discussion}.

\input{dataset.tex}

\subsection{Setup}
We train for 25 epochs with batch size 16, learning rate $1\times 10^{-4}$, and weight decay $1\times 10^{-4}$. We evaluate using accuracy and macro-F1. In addition, we plot the confusion matrix for qualitative analysis.

\subsection{Results}
Table~\ref{tab:metrics} reports the main quantitative results. Figure~\ref{fig:cm} visualizes common confusions between classes.

\begin{table}[t]
\centering
\caption{Evaluation metrics on the test split.}
\label{tab:metrics}
\begin{tabular}{lcc}
\toprule
Model & Accuracy & Macro-F1 \\
\midrule
MobileNetV2 (transfer learning) & \ResultAccuracy{} & \ResultMacroFone{} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{\ResultConfusionMatrix{}}
\caption{Confusion matrix on the test split.}
\label{fig:cm}
\end{figure}

\subsection{Ablation study}
\input{ablation.tex}

\section{Discussion}
\label{sec:discussion}
The method is simple and efficient, and it works well when classes have distinguishable visual patterns. As shown in Table~\ref{tab:ablation}, fine-tuning the backbone improves performance compared with training only a classifier head, which is expected because meme templates can differ by subtle facial contours and texture patterns. Limitations include sensitivity to dataset bias (e.g., specific sources or styles), label ambiguity (some memes resemble multiple templates), and potential overfitting when the dataset is small. Future work can explore stronger augmentations, improved data collection, and evaluating on a larger held-out set.

\section{Conclusion}
We implemented a meme template classifier based on MobileNetV2 transfer learning. The pipeline includes dataset preparation, training, evaluation, and visualization. The results demonstrate that a lightweight pretrained model can achieve reasonable performance on a small dataset.

\section{Description of member contributions}
Zhou Ziqi: dataset collection/cleaning; method explanation and presentation slides.\\
Wu RenYu: model training; evaluation and visualization; code integration.

\section{Reproducibility}
To reproduce the reported results, prepare a dataset in ImageFolder format and run the provided scripts:
\begin{itemize}
\item Quick TA test: \texttt{bash scripts/quick\_demo.sh}
\item Full experiments + ablation (updates the auto-filled tables): \texttt{bash scripts/run\_full\_experiments.sh}
\item Manual evaluation: \texttt{python3 eval.py --data\_dir data --weights pretrained/mobilenetv2\_meme\_best.pt}
\end{itemize}
All evaluation artifacts are written under \texttt{outputs/}.

\begin{thebibliography}{1}
\bibliographystyle{IEEEtran}
\bibitem{mobilenetv2}
M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, ``MobileNetV2: Inverted Residuals and Linear Bottlenecks,'' in \textit{CVPR}, 2018.
\bibitem{resnet}
K. He, X. Zhang, S. Ren, and J. Sun, ``Deep Residual Learning for Image Recognition,'' in \textit{CVPR}, 2016.
\bibitem{efficientnet}
M. Tan and Q. Le, ``EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,'' in \textit{ICML}, 2019.
\bibitem{siamese}
G. Koch, R. Zemel, and R. Salakhutdinov, ``Siamese Neural Networks for One-shot Image Recognition,'' in \textit{ICML Deep Learning Workshop}, 2015.
\bibitem{vit}
A. Dosovitskiy \textit{et al.}, ``An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,'' in \textit{ICLR}, 2021.
\bibitem{clip}
A. Radford \textit{et al.}, ``Learning Transferable Visual Models From Natural Language Supervision,'' in \textit{ICML}, 2021.
\bibitem{torchvision}
TorchVision, ``PyTorch domain library for computer vision,'' \url{https://github.com/pytorch/vision}.
\end{thebibliography}

\end{document}
